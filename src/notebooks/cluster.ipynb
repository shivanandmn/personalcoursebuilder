{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/sigmoid/miniconda3/envs/pcb/lib/python3.11/site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sigmoid/miniconda3/envs/pcb/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/sigmoid/miniconda3/envs/pcb/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sigmoid/miniconda3/envs/pcb/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sigmoid/miniconda3/envs/pcb/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sigmoid/miniconda3/envs/pcb/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.54.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 kiwisolver-1.4.7 matplotlib-3.9.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence-transformers scikit-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int32(4): ['Probability', 'Statistics', 'Programming', 'R'], np.int32(3): ['Linear Algebra', 'NLP tasks', 'Python', 'TensorFlow', 'PyTorch', 'scikit-learn'], np.int32(1): ['Machine Learning', 'Topic modelling', 'Entity Extraction', 'Summarization', 'Sentiment analysis', 'Object detection', 'Image segmentation', 'Image classification'], np.int32(0): ['AWS', 'MLOps'], np.int32(2): ['Azure', 'Google Cloud', 'Cloud AI services', 'Cloud AI tools']}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# List of skills\n",
    "skills = [\n",
    "    \"Probability\",\n",
    "    \"Statistics\",\n",
    "    \"Linear Algebra\",\n",
    "    \"Programming\",\n",
    "    \"Machine Learning\",\n",
    "    \"NLP tasks\",\n",
    "    \"Topic modelling\",\n",
    "    \"Entity Extraction\",\n",
    "    \"Summarization\",\n",
    "    \"Sentiment analysis\",\n",
    "    \"Object detection\",\n",
    "    \"Image segmentation\",\n",
    "    \"Image classification\",\n",
    "    \"AWS\",\n",
    "    \"Azure\",\n",
    "    \"Google Cloud\",\n",
    "    \"Cloud AI services\",\n",
    "    \"Cloud AI tools\",\n",
    "    \"Python\",\n",
    "    \"R\",\n",
    "    \"TensorFlow\",\n",
    "    \"PyTorch\",\n",
    "    \"scikit-learn\",\n",
    "    \"MLOps\",\n",
    "]\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "\n",
    "# Convert skills into embeddings\n",
    "embeddings = model.encode(skills)\n",
    "\n",
    "# Use KMeans clustering\n",
    "n_clusters = 5  # You can adjust the number of clusters based on your needs\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Create a dictionary to store the clusters\n",
    "cluster_dict = {}\n",
    "\n",
    "# Iterate through the clusters and organize skills into the dictionary\n",
    "for skill, cluster_label in zip(skills, kmeans.labels_):\n",
    "    if cluster_label not in cluster_dict:\n",
    "        cluster_dict[cluster_label] = []\n",
    "    cluster_dict[cluster_label].append(skill)\n",
    "\n",
    "# Display the clusters as a dictionary\n",
    "print(cluster_dict)\n",
    "# {\n",
    "#     np.int32(4): [\"Probability\", \"Statistics\", \"Linear Algebra\", \"R\"],\n",
    "#     np.int32(3): [\n",
    "#         \"Programming\",\n",
    "#         \"Machine Learning\",\n",
    "#         \"Topic modelling\",\n",
    "#         \"Summarization\",\n",
    "#         \"Python\",\n",
    "#         \"TensorFlow\",\n",
    "#         \"PyTorch\",\n",
    "#         \"scikit-learn\",\n",
    "#         \"MLOps\",\n",
    "#     ],\n",
    "#     np.int32(0): [\"NLP tasks\", \"AWS\"],\n",
    "#     np.int32(1): [\n",
    "#         \"Entity Extraction\",\n",
    "#         \"Sentiment analysis\",\n",
    "#         \"Object detection\",\n",
    "#         \"Image segmentation\",\n",
    "#         \"Image classification\",\n",
    "#     ],\n",
    "#     np.int32(2): [\"Azure\", \"Google Cloud\", \"Cloud AI services\", \"Cloud AI tools\"],\n",
    "# }\n",
    "# {\n",
    "#     np.int32(4): [\"Probability\", \"Statistics\", \"Programming\", \"R\"],\n",
    "#     np.int32(3): [\n",
    "#         \"Linear Algebra\",\n",
    "#         \"NLP tasks\",\n",
    "#         \"Python\",\n",
    "#         \"TensorFlow\",\n",
    "#         \"PyTorch\",\n",
    "#         \"scikit-learn\",\n",
    "#     ],\n",
    "#     np.int32(1): [\n",
    "#         \"Machine Learning\",\n",
    "#         \"Topic modelling\",\n",
    "#         \"Entity Extraction\",\n",
    "#         \"Summarization\",\n",
    "#         \"Sentiment analysis\",\n",
    "#         \"Object detection\",\n",
    "#         \"Image segmentation\",\n",
    "#         \"Image classification\",\n",
    "#     ],\n",
    "#     np.int32(0): [\"AWS\", \"MLOps\"],\n",
    "#     np.int32(2): [\"Azure\", \"Google Cloud\", \"Cloud AI services\", \"Cloud AI tools\"],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of clusters determined by the Silhouette Score is: 10\n",
      "Optimal Clusters and Corresponding Skills:\n",
      "{np.int32(4): ['Probability', 'Statistics', 'Programming', 'Machine Learning'], np.int32(2): ['Linear Algebra', 'Summarization'], np.int32(7): ['NLP tasks', 'Topic modelling', 'Sentiment analysis'], np.int32(8): ['Entity Extraction'], np.int32(1): ['Object detection', 'Image segmentation', 'Image classification'], np.int32(0): ['AWS', 'MLOps'], np.int32(3): ['Azure', 'Google Cloud'], np.int32(9): ['Cloud AI services', 'Cloud AI tools'], np.int32(5): ['Python', 'TensorFlow', 'PyTorch', 'scikit-learn'], np.int32(6): ['R']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{np.int32(5): ['Probability', 'Statistics'],\n",
       " np.int32(4): ['Linear Algebra', 'R'],\n",
       " np.int32(3): ['Programming',\n",
       "  'Python',\n",
       "  'TensorFlow',\n",
       "  'PyTorch',\n",
       "  'scikit-learn'],\n",
       " np.int32(1): ['Machine Learning',\n",
       "  'Object detection',\n",
       "  'Image segmentation',\n",
       "  'Image classification'],\n",
       " np.int32(2): ['NLP tasks',\n",
       "  'Topic modelling',\n",
       "  'Entity Extraction',\n",
       "  'Summarization',\n",
       "  'Sentiment analysis'],\n",
       " np.int32(0): ['AWS',\n",
       "  'Azure',\n",
       "  'Google Cloud',\n",
       "  'Cloud AI services',\n",
       "  'Cloud AI tools',\n",
       "  'MLOps']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# List of skills\n",
    "skills = [\n",
    "    \"Probability\",\n",
    "    \"Statistics\",\n",
    "    \"Linear Algebra\",\n",
    "    \"Programming\",\n",
    "    \"Machine Learning\",\n",
    "    \"NLP tasks\",\n",
    "    \"Topic modelling\",\n",
    "    \"Entity Extraction\",\n",
    "    \"Summarization\",\n",
    "    \"Sentiment analysis\",\n",
    "    \"Object detection\",\n",
    "    \"Image segmentation\",\n",
    "    \"Image classification\",\n",
    "    \"AWS\",\n",
    "    \"Azure\",\n",
    "    \"Google Cloud\",\n",
    "    \"Cloud AI services\",\n",
    "    \"Cloud AI tools\",\n",
    "    \"Python\",\n",
    "    \"R\",\n",
    "    \"TensorFlow\",\n",
    "    \"PyTorch\",\n",
    "    \"scikit-learn\",\n",
    "    \"MLOps\",\n",
    "]\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert skills into embeddings\n",
    "embeddings = model.encode(skills)\n",
    "# Find the optimal number of clusters using the Silhouette Score\n",
    "silhouette_scores = []\n",
    "max_clusters = 10  # You can adjust this as needed\n",
    "\n",
    "for i in range(2, max_clusters + 1):  # Silhouette score can't be computed for 1 cluster\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Find the number of clusters with the highest Silhouette Score\n",
    "optimal_clusters = (\n",
    "    silhouette_scores.index(max(silhouette_scores)) + 2\n",
    ")  # Because we started from 2 clusters\n",
    "\n",
    "print(\n",
    "    f\"The optimal number of clusters determined by the Silhouette Score is: {optimal_clusters}\"\n",
    ")\n",
    "\n",
    "# Run KMeans with the optimal number of clusters\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_clusters, random_state=0)\n",
    "kmeans_optimal.fit(embeddings)\n",
    "\n",
    "# Assign each skill to a cluster\n",
    "clusters = kmeans_optimal.labels_\n",
    "\n",
    "# Create a dictionary to display the clustered skills\n",
    "cluster_dict = {}\n",
    "for skill, cluster_label in zip(skills, clusters):\n",
    "    if cluster_label not in cluster_dict:\n",
    "        cluster_dict[cluster_label] = []\n",
    "    cluster_dict[cluster_label].append(skill)\n",
    "\n",
    "# Print the clusters\n",
    "print(\"Optimal Clusters and Corresponding Skills:\")\n",
    "print(cluster_dict)\n",
    "\n",
    "# paraphrase-MiniLM-L12-v2\n",
    "{\n",
    "    np.int32(4): [\"Probability\", \"Statistics\", \"R\"],\n",
    "    np.int32(3): [\"Linear Algebra\", \"TensorFlow\", \"MLOps\"],\n",
    "    np.int32(8): [\"Programming\", \"Python\"],\n",
    "    np.int32(1): [\n",
    "        \"Machine Learning\",\n",
    "        \"Entity Extraction\",\n",
    "        \"Object detection\",\n",
    "        \"Image segmentation\",\n",
    "        \"Image classification\",\n",
    "    ],\n",
    "    np.int32(5): [\"NLP tasks\", \"PyTorch\", \"scikit-learn\"],\n",
    "    np.int32(7): [\"Topic modelling\", \"Sentiment analysis\"],\n",
    "    np.int32(6): [\"Summarization\"],\n",
    "    np.int32(0): [\"AWS\"],\n",
    "    np.int32(2): [\"Azure\", \"Google Cloud\", \"Cloud AI services\", \"Cloud AI tools\"],\n",
    "}\n",
    "\n",
    "###all-MiniLM-L6-v2\n",
    "{\n",
    "    np.int32(5): [\"Probability\", \"Statistics\"],\n",
    "    np.int32(4): [\"Linear Algebra\", \"R\"],\n",
    "    np.int32(3): [\"Programming\", \"Python\", \"TensorFlow\", \"PyTorch\", \"scikit-learn\"],\n",
    "    np.int32(1): [\n",
    "        \"Machine Learning\",\n",
    "        \"Object detection\",\n",
    "        \"Image segmentation\",\n",
    "        \"Image classification\",\n",
    "    ],\n",
    "    np.int32(2): [\n",
    "        \"NLP tasks\",\n",
    "        \"Topic modelling\",\n",
    "        \"Entity Extraction\",\n",
    "        \"Summarization\",\n",
    "        \"Sentiment analysis\",\n",
    "    ],\n",
    "    np.int32(0): [\n",
    "        \"AWS\",\n",
    "        \"Azure\",\n",
    "        \"Google Cloud\",\n",
    "        \"Cloud AI services\",\n",
    "        \"Cloud AI tools\",\n",
    "        \"MLOps\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# all-distilroberta-v1\n",
    "{\n",
    "    np.int32(4): [\"Probability\", \"Statistics\", \"Programming\", \"Machine Learning\"],\n",
    "    np.int32(2): [\"Linear Algebra\", \"Summarization\"],\n",
    "    np.int32(7): [\"NLP tasks\", \"Topic modelling\", \"Sentiment analysis\"],\n",
    "    np.int32(8): [\"Entity Extraction\"],\n",
    "    np.int32(1): [\"Object detection\", \"Image segmentation\", \"Image classification\"],\n",
    "    np.int32(0): [\"AWS\", \"MLOps\"],\n",
    "    np.int32(3): [\"Azure\", \"Google Cloud\"],\n",
    "    np.int32(9): [\"Cloud AI services\", \"Cloud AI tools\"],\n",
    "    np.int32(5): [\"Python\", \"TensorFlow\", \"PyTorch\", \"scikit-learn\"],\n",
    "    np.int32(6): [\"R\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int64(1): ['Probability', 'Statistics', 'Linear Algebra', 'Machine Learning', 'Summarization', 'R', 'scikit-learn'], np.int64(0): ['Programming', 'NLP tasks', 'Topic modelling', 'Sentiment analysis', 'AWS'], np.int64(2): ['Entity Extraction', 'Object detection', 'Image segmentation', 'Image classification'], np.int64(3): ['Azure', 'Google Cloud', 'Cloud AI services', 'Cloud AI tools'], np.int64(4): ['Python', 'TensorFlow', 'PyTorch', 'MLOps']}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# List of skills\n",
    "skills = [\n",
    "    \"Probability\",\n",
    "    \"Statistics\",\n",
    "    \"Linear Algebra\",\n",
    "    \"Programming\",\n",
    "    \"Machine Learning\",\n",
    "    \"NLP tasks\",\n",
    "    \"Topic modelling\",\n",
    "    \"Entity Extraction\",\n",
    "    \"Summarization\",\n",
    "    \"Sentiment analysis\",\n",
    "    \"Object detection\",\n",
    "    \"Image segmentation\",\n",
    "    \"Image classification\",\n",
    "    \"AWS\",\n",
    "    \"Azure\",\n",
    "    \"Google Cloud\",\n",
    "    \"Cloud AI services\",\n",
    "    \"Cloud AI tools\",\n",
    "    \"Python\",\n",
    "    \"R\",\n",
    "    \"TensorFlow\",\n",
    "    \"PyTorch\",\n",
    "    \"scikit-learn\",\n",
    "    \"MLOps\",\n",
    "]\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert skills into embeddings\n",
    "embeddings = model.encode(skills)\n",
    "\n",
    "# Use Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(\n",
    "    n_clusters=5\n",
    ")  # Adjust the number of clusters if needed\n",
    "labels = agg_clustering.fit_predict(embeddings)\n",
    "\n",
    "# Create a dictionary to store the clusters\n",
    "cluster_dict = {}\n",
    "\n",
    "# Organize skills into the dictionary based on the Agglomerative Clustering results\n",
    "for skill, cluster_label in zip(skills, labels):\n",
    "    if cluster_label not in cluster_dict:\n",
    "        cluster_dict[cluster_label] = []\n",
    "    cluster_dict[cluster_label].append(skill)\n",
    "\n",
    "# Display the clusters as a dictionary\n",
    "print(cluster_dict)\n",
    "{\n",
    "    np.int64(1): [\n",
    "        \"Probability\",\n",
    "        \"Statistics\",\n",
    "        \"Linear Algebra\",\n",
    "        \"Machine Learning\",\n",
    "        \"Summarization\",\n",
    "        \"R\",\n",
    "        \"scikit-learn\",\n",
    "    ],\n",
    "    np.int64(0): [\n",
    "        \"Programming\",\n",
    "        \"NLP tasks\",\n",
    "        \"Topic modelling\",\n",
    "        \"Sentiment analysis\",\n",
    "        \"AWS\",\n",
    "    ],\n",
    "    np.int64(2): [\n",
    "        \"Entity Extraction\",\n",
    "        \"Object detection\",\n",
    "        \"Image segmentation\",\n",
    "        \"Image classification\",\n",
    "    ],\n",
    "    np.int64(3): [\"Azure\", \"Google Cloud\", \"Cloud AI services\", \"Cloud AI tools\"],\n",
    "    np.int64(4): [\"Python\", \"TensorFlow\", \"PyTorch\", \"MLOps\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int64(-1): ['Probability', 'Statistics', 'Linear Algebra', 'Programming', 'Machine Learning', 'NLP tasks', 'Topic modelling', 'Entity Extraction', 'Summarization', 'Sentiment analysis', 'AWS', 'Python', 'R', 'TensorFlow', 'PyTorch', 'scikit-learn', 'MLOps'], np.int64(0): ['Object detection', 'Image segmentation', 'Image classification'], np.int64(1): ['Azure', 'Google Cloud', 'Cloud AI services', 'Cloud AI tools']}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# List of skills (as before)\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert skills into embeddings\n",
    "embeddings = model.encode(skills)\n",
    "\n",
    "# Use DBSCAN for clustering\n",
    "dbscan = DBSCAN(\n",
    "    eps=0.5, min_samples=2, metric=\"cosine\"\n",
    ")  # Adjust eps and min_samples for different results\n",
    "labels = dbscan.fit_predict(embeddings)\n",
    "\n",
    "# Create a dictionary to store the clusters\n",
    "cluster_dict = {}\n",
    "\n",
    "# Organize skills into the dictionary based on DBSCAN clustering results\n",
    "for skill, cluster_label in zip(skills, labels):\n",
    "    if cluster_label not in cluster_dict:\n",
    "        cluster_dict[cluster_label] = []\n",
    "    cluster_dict[cluster_label].append(skill)\n",
    "\n",
    "# Display the clusters as a dictionary\n",
    "print(cluster_dict)\n",
    "{\n",
    "    np.int64(-1): [\n",
    "        \"Probability\",\n",
    "        \"Statistics\",\n",
    "        \"Linear Algebra\",\n",
    "        \"Programming\",\n",
    "        \"Machine Learning\",\n",
    "        \"NLP tasks\",\n",
    "        \"Topic modelling\",\n",
    "        \"Entity Extraction\",\n",
    "        \"Summarization\",\n",
    "        \"Sentiment analysis\",\n",
    "        \"AWS\",\n",
    "        \"Python\",\n",
    "        \"R\",\n",
    "        \"TensorFlow\",\n",
    "        \"PyTorch\",\n",
    "        \"scikit-learn\",\n",
    "        \"MLOps\",\n",
    "    ],\n",
    "    np.int64(0): [\"Object detection\", \"Image segmentation\", \"Image classification\"],\n",
    "    np.int64(1): [\"Azure\", \"Google Cloud\", \"Cloud AI services\", \"Cloud AI tools\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int32(3): ['Probability', 'Statistics', 'Machine Learning', 'Topic modelling', 'Summarization', 'Sentiment analysis'], np.int32(0): ['Linear Algebra', 'Python', 'R', 'TensorFlow', 'PyTorch', 'scikit-learn', 'MLOps'], np.int32(4): ['Programming', 'NLP tasks', 'AWS'], np.int32(2): ['Entity Extraction', 'Object detection', 'Image segmentation', 'Image classification'], np.int32(1): ['Azure', 'Google Cloud', 'Cloud AI services', 'Cloud AI tools']}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# List of skills (as before)\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert skills into embeddings\n",
    "embeddings = model.encode(skills)\n",
    "\n",
    "# Use Spectral Clustering\n",
    "spectral = SpectralClustering(\n",
    "    n_clusters=5, affinity=\"nearest_neighbors\", random_state=0\n",
    ")\n",
    "labels = spectral.fit_predict(embeddings)\n",
    "\n",
    "# Create a dictionary to store the clusters\n",
    "cluster_dict = {}\n",
    "\n",
    "# Organize skills into the dictionary based on the Spectral Clustering results\n",
    "for skill, cluster_label in zip(skills, labels):\n",
    "    if cluster_label not in cluster_dict:\n",
    "        cluster_dict[cluster_label] = []\n",
    "    cluster_dict[cluster_label].append(skill)\n",
    "\n",
    "# Display the clusters as a dictionary\n",
    "print(cluster_dict)\n",
    "{\n",
    "    np.int32(3): [\n",
    "        \"Probability\",\n",
    "        \"Statistics\",\n",
    "        \"Machine Learning\",\n",
    "        \"Topic modelling\",\n",
    "        \"Summarization\",\n",
    "        \"Sentiment analysis\",\n",
    "    ],\n",
    "    np.int32(0): [\n",
    "        \"Linear Algebra\",\n",
    "        \"Python\",\n",
    "        \"R\",\n",
    "        \"TensorFlow\",\n",
    "        \"PyTorch\",\n",
    "        \"scikit-learn\",\n",
    "        \"MLOps\",\n",
    "    ],\n",
    "    np.int32(4): [\"Programming\", \"NLP tasks\", \"AWS\"],\n",
    "    np.int32(2): [\n",
    "        \"Entity Extraction\",\n",
    "        \"Object detection\",\n",
    "        \"Image segmentation\",\n",
    "        \"Image classification\",\n",
    "    ],\n",
    "    np.int32(1): [\"Azure\", \"Google Cloud\", \"Cloud AI services\", \"Cloud AI tools\"],\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
